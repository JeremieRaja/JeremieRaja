{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8b948d",
   "metadata": {},
   "source": [
    "1. Prétraitement du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce85b20",
   "metadata": {},
   "source": [
    "Téléchargement du dataset contenant les balises <AbstractText> segmentée en phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58038697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymed import PubMed\n",
    "pubmed = PubMed(tool=\"MyTool\", email=\"myemail@example.com\")\n",
    "results = pubmed.query(\"diabetis\", max_results=15)\n",
    "sentences=[]#liste des phrases\n",
    "corpus=' '\n",
    "for article in results:\n",
    "    sentences.append(article.title)#chargement des données du dataset dans la liste créee\n",
    "for i in sentences:\n",
    "    corpus=corpus + ''.join(i)#Conversion de la liste des phrases en chaine de caractères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4327791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Referable Diabetic Retinopathy Prediction Algorithm Applied to a Population of 120,389 Type 2 Diabetics over 11 Years Follow-Up.Severe hypoglycemia and hypoglycemia awareness are associated with preclinical atherosclerosis in patients with type 1 diabetes without an estimated high cardiovascular risk.Adherence to an energy-restricted Mediterranean diet is associated with the presence and burden of carotid atherosclerosis in people with type 1 diabetes.Survivin/BIRC5 as a novel molecular effector at the crossroads of glucose metabolism and radioresistance in head and neck squamous cell carcinoma.Midnight Cortisol is Associated with Changes in Systolic Blood Pressure and Diabetic Neuropathy in Subjects with Type\\xa01 Diabetes Undergoing Simultaneous Kidney-Pancreas Transplantation.Rapid Reduction of HbA1c and Early Worsening of Diabetic Retinopathy: A Real-world Population-Based Study in Subjects With Type 2 Diabetes.Nuclear Magnetic Resonance-Based Lipidomics in the Assessment of Cardiometabolic Risk in Type\\xa01 Diabetes: An Exploratory Analysis.Obesity and related comorbidities in a large population-based cohort of subjects with type 1 diabetes in Catalonia.Platinum/carbon dots nanocomposites from palm bunch hydrothermal synthesis as highly efficient peroxidase mimics for ultra-low HDiabetic retinopathy as a predictor of cardiovascular morbidity and mortality in subjects with type 2 diabetes.Holistic multi-class classification & grading of diabetic foot ulcerations from plantar thermal images using deep learning.Genetics: Is LADA just late onset type 1 diabetes?Positive Effects of a Mediterranean Diet Supplemented with Almonds on Female Adipose Tissue Biology in Severe Obesity.Glycaemia Fluctuations Improvement in Old-Age Prediabetic Subjects Consuming a Quinoa-Based Diet: A Pilot Study.Molecular Markers in the Diagnosis of Thyroid Cancer in Indeterminate Thyroid Nodules.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3db14",
   "metadata": {},
   "source": [
    "2. Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3232747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Probabilities:\n",
      "           Unigram  Probability\n",
      "0        referable     0.003846\n",
      "1         diabetic     0.019231\n",
      "2      retinopathy     0.007692\n",
      "3       prediction     0.003846\n",
      "4        algorithm     0.003846\n",
      "..             ...          ...\n",
      "158      diagnosis     0.003846\n",
      "159        thyroid     0.007692\n",
      "160         cancer     0.003846\n",
      "161  indeterminate     0.003846\n",
      "162       nodules.     0.003846\n",
      "\n",
      "[163 rows x 2 columns]\n",
      "\n",
      "Bigram Probabilities:\n",
      "                        Bigram  Probability\n",
      "0        (referable, diabetic)     0.003861\n",
      "1      (diabetic, retinopathy)     0.007722\n",
      "2    (retinopathy, prediction)     0.003861\n",
      "3      (prediction, algorithm)     0.003861\n",
      "4         (algorithm, applied)     0.003861\n",
      "..                         ...          ...\n",
      "225          (thyroid, cancer)     0.003861\n",
      "226               (cancer, in)     0.003861\n",
      "227        (in, indeterminate)     0.003861\n",
      "228   (indeterminate, thyroid)     0.003861\n",
      "229        (thyroid, nodules.)     0.003861\n",
      "\n",
      "[230 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def compute_unigram_probabilities(corpus):\n",
    "    words = [word.lower() for sentence in corpus for word in sentence.split()]\n",
    "    total_words = len(words)\n",
    "    unigram_frequencies = Counter(words)\n",
    "    unigram_probabilities = {word: freq / total_words for word, freq in unigram_frequencies.items()}\n",
    "    unigram_df = pd.DataFrame(list(unigram_probabilities.items()), columns=['Unigram', 'Probability'])\n",
    "    return unigram_df\n",
    "\n",
    "def compute_bigram_probabilities(corpus):\n",
    "    words = [word.lower() for sentence in corpus for word in sentence.split()]\n",
    "    total_bigrams = len(words) - 1\n",
    "    bigrams = [(words[i], words[i + 1]) for i in range(total_bigrams)]\n",
    "    bigram_frequencies = Counter(bigrams)\n",
    "    bigram_probabilities = {bigram: freq / total_bigrams for bigram, freq in bigram_frequencies.items()}\n",
    "    bigram_df = pd.DataFrame(list(bigram_probabilities.items()), columns=['Bigram', 'Probability'])\n",
    "    return bigram_df\n",
    "\n",
    "# Compute unigram probabilities\n",
    "unigram_df = compute_unigram_probabilities(sentences)\n",
    "print(\"Unigram Probabilities:\")\n",
    "print(unigram_df)\n",
    "# Compute bigram probabilities\n",
    "bigram_df = compute_bigram_probabilities(sentences)\n",
    "print(\"\\nBigram Probabilities:\")\n",
    "print(bigram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42096abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Referable', 'JJ'), ('Diabetic', 'NNP'), ('Retinopathy', 'NNP'), ('Prediction', 'NNP'), ('Algorithm', 'NNP'), ('Applied', 'NNP'), ('to', 'TO'), ('a', 'DT'), ('Population', 'NN'), ('of', 'IN'), ('120,389', 'CD'), ('Type', 'NNP'), ('2', 'CD'), ('Diabetics', 'NNPS'), ('over', 'IN'), ('11', 'CD'), ('Years', 'NNS'), ('Follow-Up.Severe', 'JJ'), ('hypoglycemia', 'NN'), ('and', 'CC'), ('hypoglycemia', 'NN'), ('awareness', 'NN'), ('are', 'VBP'), ('associated', 'VBN'), ('with', 'IN'), ('preclinical', 'JJ'), ('atherosclerosis', 'NN'), ('in', 'IN'), ('patients', 'NNS'), ('with', 'IN'), ('type', 'JJ'), ('1', 'CD'), ('diabetes', 'NNS'), ('without', 'IN'), ('an', 'DT'), ('estimated', 'VBN'), ('high', 'JJ'), ('cardiovascular', 'JJ'), ('risk.Adherence', 'NN'), ('to', 'TO'), ('an', 'DT'), ('energy-restricted', 'JJ'), ('Mediterranean', 'NNP'), ('diet', 'NN'), ('is', 'VBZ'), ('associated', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('presence', 'NN'), ('and', 'CC'), ('burden', 'NN'), ('of', 'IN'), ('carotid', 'JJ'), ('atherosclerosis', 'NN'), ('in', 'IN'), ('people', 'NNS'), ('with', 'IN'), ('type', 'JJ'), ('1', 'CD'), ('diabetes.Survivin/BIRC5', 'NN'), ('as', 'IN'), ('a', 'DT'), ('novel', 'JJ'), ('molecular', 'JJ'), ('effector', 'NN'), ('at', 'IN'), ('the', 'DT'), ('crossroads', 'NNS'), ('of', 'IN'), ('glucose', 'JJ'), ('metabolism', 'NN'), ('and', 'CC'), ('radioresistance', 'NN'), ('in', 'IN'), ('head', 'NN'), ('and', 'CC'), ('neck', 'NN'), ('squamous', 'JJ'), ('cell', 'NN'), ('carcinoma.Midnight', 'NN'), ('Cortisol', 'NNP'), ('is', 'VBZ'), ('Associated', 'VBN'), ('with', 'IN'), ('Changes', 'NNS'), ('in', 'IN'), ('Systolic', 'NNP'), ('Blood', 'NNP'), ('Pressure', 'NNP'), ('and', 'CC'), ('Diabetic', 'NNP'), ('Neuropathy', 'NNP'), ('in', 'IN'), ('Subjects', 'NNP'), ('with', 'IN'), ('Type', 'NNP'), ('1', 'CD'), ('Diabetes', 'NNP'), ('Undergoing', 'NNP'), ('Simultaneous', 'NNP'), ('Kidney-Pancreas', 'NNP'), ('Transplantation.Rapid', 'NNP'), ('Reduction', 'NNP'), ('of', 'IN'), ('HbA1c', 'NNP'), ('and', 'CC'), ('Early', 'NNP'), ('Worsening', 'NNP'), ('of', 'IN'), ('Diabetic', 'NNP'), ('Retinopathy', 'NNP'), (':', ':'), ('A', 'DT'), ('Real-world', 'NNP'), ('Population-Based', 'JJ'), ('Study', 'NNP'), ('in', 'IN'), ('Subjects', 'NNP'), ('With', 'IN'), ('Type', 'NNP'), ('2', 'CD'), ('Diabetes.Nuclear', 'NNP'), ('Magnetic', 'NNP'), ('Resonance-Based', 'JJ'), ('Lipidomics', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('Assessment', 'NNP'), ('of', 'IN'), ('Cardiometabolic', 'NNP'), ('Risk', 'NNP'), ('in', 'IN'), ('Type', 'NNP'), ('1', 'CD'), ('Diabetes', 'NNS'), (':', ':'), ('An', 'DT'), ('Exploratory', 'NNP'), ('Analysis.Obesity', 'NNP'), ('and', 'CC'), ('related', 'JJ'), ('comorbidities', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('large', 'JJ'), ('population-based', 'JJ'), ('cohort', 'NN'), ('of', 'IN'), ('subjects', 'NNS'), ('with', 'IN'), ('type', 'JJ'), ('1', 'CD'), ('diabetes', 'NNS'), ('in', 'IN'), ('Catalonia.Platinum/carbon', 'NNP'), ('dots', 'NNS'), ('nanocomposites', 'NNS'), ('from', 'IN'), ('palm', 'NN'), ('bunch', 'NN'), ('hydrothermal', 'JJ'), ('synthesis', 'NN'), ('as', 'IN'), ('highly', 'RB'), ('efficient', 'JJ'), ('peroxidase', 'NN'), ('mimics', 'NNS'), ('for', 'IN'), ('ultra-low', 'JJ'), ('HDiabetic', 'NNP'), ('retinopathy', 'NN'), ('as', 'IN'), ('a', 'DT'), ('predictor', 'NN'), ('of', 'IN'), ('cardiovascular', 'JJ'), ('morbidity', 'NN'), ('and', 'CC'), ('mortality', 'NN'), ('in', 'IN'), ('subjects', 'NNS'), ('with', 'IN'), ('type', 'JJ'), ('2', 'CD'), ('diabetes.Holistic', 'JJ'), ('multi-class', 'NN'), ('classification', 'NN'), ('&', 'CC'), ('grading', 'NN'), ('of', 'IN'), ('diabetic', 'JJ'), ('foot', 'NN'), ('ulcerations', 'NNS'), ('from', 'IN'), ('plantar', 'NN'), ('thermal', 'JJ'), ('images', 'NNS'), ('using', 'VBG'), ('deep', 'JJ'), ('learning.Genetics', 'NNS'), (':', ':'), ('Is', 'VBZ'), ('LADA', 'NNP'), ('just', 'RB'), ('late', 'RB'), ('onset', 'VBN'), ('type', 'NN'), ('1', 'CD'), ('diabetes', 'NNS'), ('?', '.'), ('Positive', 'JJ'), ('Effects', 'NNPS'), ('of', 'IN'), ('a', 'DT'), ('Mediterranean', 'NNP'), ('Diet', 'NNP'), ('Supplemented', 'NNP'), ('with', 'IN'), ('Almonds', 'NNP'), ('on', 'IN'), ('Female', 'NNP'), ('Adipose', 'NNP'), ('Tissue', 'NNP'), ('Biology', 'NNP'), ('in', 'IN'), ('Severe', 'NNP'), ('Obesity.Glycaemia', 'NNP'), ('Fluctuations', 'NNP'), ('Improvement', 'NNP'), ('in', 'IN'), ('Old-Age', 'NNP'), ('Prediabetic', 'NNP'), ('Subjects', 'NNP'), ('Consuming', 'VBG'), ('a', 'DT'), ('Quinoa-Based', 'JJ'), ('Diet', 'NNP'), (':', ':'), ('A', 'DT'), ('Pilot', 'NNP'), ('Study.Molecular', 'NNP'), ('Markers', 'NNP'), ('in', 'IN'), ('the', 'DT'), ('Diagnosis', 'NNP'), ('of', 'IN'), ('Thyroid', 'NNP'), ('Cancer', 'NNP'), ('in', 'IN'), ('Indeterminate', 'NNP'), ('Thyroid', 'NNP'), ('Nodules', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Get POS tags for each word in the sentences\n",
    "text_words=word_tokenize(corpus)\n",
    "tagged_words=pos_tag(text_words)\n",
    "print(tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a485831f",
   "metadata": {},
   "source": [
    "3. Named Entity Recognition\n",
    "Implement NER tagging using transition matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c298e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "doc = nlp(corpus)\n",
    "\n",
    "#visualizing the entities in the sentence using the wonderfully-named displacy inside of spaCy.\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")\n",
    "\n",
    "#visualizing the entities in the sentence using a DataFrame.\n",
    "entities = [(ent.text, ent.label_, ent.lemma_) for ent in doc.ents]\n",
    "df = pd.DataFrame(entities, columns=['text', 'type', 'lemma'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03da775",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(corpus)\n",
    "df[df.type == 'ORG'].lemma.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "diabetes.target[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd6470",
   "metadata": {},
   "source": [
    "4. Topics Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6897bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "from sklearn import datasets\n",
    "# alternative writing from sklearn.datasets import fetch_20newsgroups\n",
    "# we declare certain categories on which we will do our training\n",
    "categories = ['sci.med','sci.space','sci.electronics']\n",
    "# our training model\n",
    "ng_train = datasets.fetch_20newsgroups(subset='train', \n",
    "                                       categories=categories, \n",
    "                                       remove=('headers', \n",
    "                                               'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1f74c8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am looking for current sources for lists of all the home\n",
      "medical tests currently legally available.\n",
      "I believe this trend of allowing tests at home where\n",
      "feasible, decreased medical costs by a factor of 10 or\n",
      "more and allows the patient some time and privacy to\n",
      "consider the best action from the results of such tests.\n",
      "In fact I believe home medical tests and certain basic\n",
      "tests for serious diseases such as cancer, heart disease,\n",
      "should be offered free to the American public.\n",
      "This could actually help to reduce national medical costs\n",
      "since many would have an earlier opportunity to know\n",
      "about and work toward recuperation or cure.\n",
      "Mike Romano\n",
      "\n",
      "\n",
      "++\n",
      " [ Article crossposted from comp.windows.ms ]\n",
      "[ Author was Kevin Routh ]\n",
      "[ Posted on 19 Apr 1993 12:35:55 GMT ]\n",
      "\n",
      "For your information:\n",
      "\n",
      "I hooked up my ImageWriter I to my COM1 serial port and used the C-Itoh\n",
      "8510 driver in Windows 3.1.  The cable I am using is a straight-thru\n",
      "cable connected to a Null Modem Adapter I got at Radio Shack (catalog\n",
      "#26-1496a) for $4.95.  It seems to work fine with both DOS and Windows.\n",
      "I used the following command in DOS\n",
      "\n",
      "\tC:\\DOS\\mode COM1:9600,n,8,1,p\n",
      "\n",
      "and set up the port the same way in the Windows Ports setup.\n",
      "\n",
      "the Null Modem connections are as follows:\n",
      "\n",
      "\t1\tto\t1\n",
      "\t2\tto\t3\n",
      "\t3\tto\t2\n",
      "\t4\tto\t5\n",
      "\t5\tto\t4\n",
      "\t6+8\tto\t20\n",
      "\t20\tto\t6+8\n",
      "\t7\tto\t7\n",
      "\n",
      "I printed from several applications and all seems OK.  \n",
      "\n",
      "-- \n",
      "Kevin C. Routh                     Internet:    krouth@slee01.srl.ford.com\n",
      "Ford Electronics                   IBMmail (PROFS):               USFMCTMF\n",
      "ELD IC Engineering\n",
      "17000 Rotunda Drive, B-121         Voice mail:              (313) 337-5136\n",
      "Dearborn, MI  48121-6010           Facsimile:               (313) 248-6244\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "Kevin C. Routh                     Internet:    krouth@slee01.srl.ford.com\n",
      "Ford Electronics                   IBMmail (PROFS):               USFMCTMF\n",
      "ELD IC Engineering\n",
      "17000 Rotunda Drive, B-121         Voice mail:              (313) 337-5136\n",
      "Dearborn, MI  48121-6010           Facsimile:               (313) 248-6244\n",
      "\n",
      "\n",
      "++\n",
      " Hmm.  $1 billion, lesse... I can probably launch 100 tons to LEO at\n",
      "$200 million, in five years, which gives about 20 tons to the lunar\n",
      "surface one-way.  Say five tons of that is a return vehicle and its\n",
      "fuel, a bigger Mercury or something (might get that as low as two\n",
      "tons), leaving fifteen tons for a one-man habitat and a year's supplies?\n",
      "Gee, with that sort of mass margins I can build the systems off\n",
      "the shelf for about another hundred million tops.  That leaves\n",
      "about $700 million profit.  I like this idea 8-)  Let's see\n",
      "if you guys can push someone to make it happen 8-) 8-)\n",
      "\n",
      "[slightly seriously]\n",
      "\n",
      "\n",
      "Data has 1778 documents\n"
     ]
    }
   ],
   "source": [
    "# Data verification\n",
    "print(ng_train.data[2])\n",
    "print(\"++\\n\", ng_train.data[1504])\n",
    "print(\"++\\n\", ng_train.data[1000])\n",
    "print(\"\\n\\nData has {0:d} documents\". format(len(ng_train.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eec54811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Pre-process all the words in the document, including deleting empty words.\n",
    "#Render all text in lower case.\n",
    "# the countvectorizer allows us to carry out several pre-processing operations at once\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,2),\n",
    "                                   stop_words='english',\n",
    "                                   token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                                   lowercase=True,\n",
    "                                   max_features=1000)\n",
    "X  = count_vectorizer.fit_transform(ng_train.data) # X is now our transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b82b972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50943573 0.0461408  0.44442347]\n",
      "['sci.med', 'sci.space', 'sci.electronics']\n"
     ]
    }
   ],
   "source": [
    "#Using sklearn, create an LDA model with 3 subjects\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation (3, random_state=42, learning_method='online')\n",
    "data = lda.fit_transform(X)\n",
    "print(data[0])\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d039123e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another fish to check out is Richard Rast -- he works\n",
      "for Lockheed Missiles, but is on-site at NASA Johnson.\n",
      "\n",
      "Nick Johnson at Kaman Sciences in Colo. Spgs and his\n",
      "friend, Darren McKnight at Kaman in Alexandria, VA.\n",
      "\n",
      "Good luck.\n",
      "\n",
      "R. Landis\n"
     ]
    }
   ],
   "source": [
    "print(ng_train.data[0]) # This document is 50% topic 3!  according to statitics but the content of this document shows us something else"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8630117",
   "metadata": {},
   "source": [
    "5. Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the pre-trained MarianMT model and tokenizer for English to French\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "def translate(sentences, target_language='fr'):\n",
    "    translated = []\n",
    "    for sentence in sentences:\n",
    "        translated_text = model.generate(**tokenizer(sentence, return_tensors=\"pt\", padding=True))\n",
    "        translated.append(tokenizer.decode(translated_text[0], skip_special_tokens=True))\n",
    "    return translated\n",
    "\n",
    "# Example usage\n",
    "for translation in translations:\n",
    "    print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4777187",
   "metadata": {},
   "source": [
    "Front-end: ----------------------voir le fichier app.py---------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
